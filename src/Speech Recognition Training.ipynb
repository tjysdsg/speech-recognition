{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "data_path = \"aurora_digits\"\n",
    "digit_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'O', 'Z']\n",
    "\n",
    "filename_index_map = {\n",
    "    '1': 0,\n",
    "    '2': 1,\n",
    "    '3': 2,\n",
    "    '4': 3,\n",
    "    '5': 4,\n",
    "    '6': 5,\n",
    "    '7': 6,\n",
    "    '8': 7,\n",
    "    '9': 8,\n",
    "    'O': 9,\n",
    "    'Z': 10,\n",
    "}\n",
    "\n",
    "isolated_model_path = 'isolated_models'\n",
    "continuous_model_path = 'continuous_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(isolated_model_path)\n",
    "except:\n",
    "    print(\"Warnings:\", isolated_model_path, \"already exits?\")\n",
    "    \n",
    "try:\n",
    "    os.mkdir(continuous_model_path)\n",
    "except:\n",
    "    print(\"Warnings:\", continuous_model_path, \"already exits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature.py\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import dct\n",
    "import math\n",
    "\n",
    "\n",
    "def segment(sig, sample_rate=16000, frame_len=0.025, frame_step=0.01):\n",
    "    slen = sig.size\n",
    "    frame_len1 = int(frame_len * sample_rate)\n",
    "    frame_step1 = int(frame_step * sample_rate)\n",
    "    num_frames = 1\n",
    "    if slen > frame_len:\n",
    "        num_frames = math.ceil(slen / frame_step1)\n",
    "\n",
    "    final_len = int((num_frames - 1) * frame_step1 + frame_len1)\n",
    "\n",
    "    pad_sig = np.concatenate([sig, np.zeros(final_len - slen)])\n",
    "    frames = np.zeros((num_frames, frame_len1))\n",
    "    for i in range(num_frames):\n",
    "        frames[i, :] = pad_sig[i * frame_step1:i * frame_step1 + frame_len1]\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def zero_padding(frames, frame_len=None):\n",
    "    width = frames.shape[1]\n",
    "    height = frames.shape[0]\n",
    "\n",
    "    # if frame_len is not specified, find the next power of 2\n",
    "    if frame_len is None:\n",
    "        frame_len = 1 << (width - 1).bit_length()\n",
    "\n",
    "    pad_len = frame_len - width\n",
    "    pad_len_left = pad_len // 2\n",
    "    pad_len_right = pad_len - pad_len_left\n",
    "\n",
    "    f = np.zeros((frames.shape[0], frame_len))\n",
    "    for i in range(0, height):\n",
    "        f[i, pad_len_left:frame_len - pad_len_right] = frames[i, :]\n",
    "    return f\n",
    "\n",
    "\n",
    "def mfcc_features(path_file, frame_size=0.025, frame_stride=0.01, low_freq=80, high_freq=None):\n",
    "    sample_rate, signal = wavfile.read(path_file)\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "\n",
    "    frames = segment(emphasized_signal, sample_rate, frame_size, frame_stride)\n",
    "    frames = zero_padding(frames)\n",
    "\n",
    "    # hamming window\n",
    "    frames *= np.hamming(frames.shape[1])\n",
    "\n",
    "    NFFT = 512\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n",
    "    pow_frames = ((1.0 / NFFT) * (mag_frames ** 2))  # Power Spectrum\n",
    "\n",
    "    nfilt = 40\n",
    "    high_freq = high_freq or sample_rate / 2\n",
    "    low_freq_mel = (2595 * np.log10(1 + low_freq / 700))  # Convert Hz to Mel\n",
    "    high_freq_mel = (2595 * np.log10(1 + high_freq / 700))  # Convert Hz to Mel\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "    hz_points = (700 * (10 ** (mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])  # left\n",
    "        f_m = int(bin[m])  # center\n",
    "        f_m_plus = int(bin[m + 1])  # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\n",
    "    filter_banks = np.log10(filter_banks)\n",
    "\n",
    "    num_ceps = 13\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1: (num_ceps + 1)]\n",
    "    return filter_banks, mfcc\n",
    "\n",
    "\n",
    "def standardize(data):\n",
    "    data -= (np.mean(data, axis=0))\n",
    "    data = data / np.std(data, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtw.py\n",
    "import numpy as np\n",
    "from math import isinf\n",
    "\n",
    "\n",
    "def dtw(x, y, dist_fun, transitions, variance=None, beam=np.inf):\n",
    "    \"\"\"\n",
    "    :param x: an input\n",
    "    :param y: a template\n",
    "    :param dist_fun: function used to calculate distance between two nodes.\n",
    "    :param transitions: transition matrix, where transitions[i,j] represents the cost for transiting from jth to ith\n",
    "        state.\n",
    "    :param variance: covariance matrix of the segments in the templates.\n",
    "    :param beam: beam size of pruning.\n",
    "    :return: costs: cost matrix.\n",
    "            path: reversed path (from end to start). [[x_n,y_n],...,[x_2,y_2],[x_1,y_1]]\n",
    "    \"\"\"\n",
    "    # initialize cost matrix\n",
    "    col_count = len(x)\n",
    "    row_count = len(y)\n",
    "    assert col_count > 1 and row_count > 1\n",
    "    costs = np.full((row_count, col_count), np.inf)\n",
    "    path_matrix = np.full((row_count, col_count, 2), np.inf, dtype=np.int)\n",
    "\n",
    "    j = 0\n",
    "    while True:\n",
    "        if j == col_count:\n",
    "            break\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i == row_count:\n",
    "                break\n",
    "            if i == 0 and j == 0:\n",
    "                if variance is None:\n",
    "                    costs[0, 0] = dist_fun(x[j], y[i])\n",
    "                else:\n",
    "                    costs[0, 0] = dist_fun(x[j], y[i], variance[i])\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            prev_costs = []\n",
    "            from_pts = []\n",
    "            for origin in range(transitions.shape[1]):\n",
    "                # cost is set to -1 if it is excluded from beam search\n",
    "                if costs[origin, j - 1] == -1:\n",
    "                    # set it back to np.inf so that it is more convenient to be recognized by pathfinder\n",
    "                    costs[origin, j - 1] = np.inf\n",
    "                else:\n",
    "                    prev_costs.append(transitions[i, origin] + costs[origin, j - 1])\n",
    "                    from_pts.append([origin, j - 1])\n",
    "\n",
    "            min_i = np.argmin(prev_costs)\n",
    "            origin = from_pts[min_i]\n",
    "            path_matrix[i, j] = origin\n",
    "            if variance is None:\n",
    "                curr_cost = prev_costs[min_i] + dist_fun(x[j], y[i])\n",
    "            else:\n",
    "                curr_cost = prev_costs[min_i] + dist_fun(x[j], y[i], variance[i])\n",
    "            costs[i, j] = min(costs[i, j], curr_cost)\n",
    "            i += 1\n",
    "        if not isinf(beam):\n",
    "            sort_idx = np.argsort(costs[:, j].flatten())\n",
    "            # cost is set to -1 if it is excluded from beam search\n",
    "            exclude_idx = sort_idx[beam:]\n",
    "            for i in exclude_idx:\n",
    "                if not isinf(costs[i, j]):\n",
    "                    costs[i, j] = -1\n",
    "        j += 1\n",
    "    # find the path\n",
    "    i = row_count - 1\n",
    "    j = col_count - 1\n",
    "    path = []\n",
    "    while i != 0 or j != 0:\n",
    "        i, j = path_matrix[i, j]\n",
    "        path.append([i, j])\n",
    "    return costs, np.array(path)\n",
    "\n",
    "\n",
    "def dtw1(x, states, transitions, beam=np.inf):\n",
    "    \"\"\"\n",
    "    :param x: an input\n",
    "    :param states: a list of states (GMM object).\n",
    "    :param transitions: transition matrix, where transitions[i,j] represents the cost for transiting from jth to ith\n",
    "        state.\n",
    "    :param beam: beam size of pruning.\n",
    "    :return: costs: cost matrix.\n",
    "            path: reversed path (from end to start). [[x_n,y_n],...,[x_2,y_2],[x_1,y_1]]\n",
    "    \"\"\"\n",
    "    # initialize cost matrix\n",
    "    col_count = len(x)\n",
    "    row_count = len(states)\n",
    "    costs = np.full((row_count, col_count), np.inf)\n",
    "    path_matrix = np.full((row_count, col_count, 2), np.inf, dtype=np.int)\n",
    "\n",
    "    j = 0\n",
    "    while True:\n",
    "        if j == col_count:\n",
    "            break\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i == row_count:\n",
    "                break\n",
    "            if i == 0 and j == 0:\n",
    "                costs[0, 0] = states[i].evaluate(x[j])\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            prev_costs = []\n",
    "            from_pts = []\n",
    "            for origin in range(transitions.shape[1]):\n",
    "                # cost is set to -1 if it is excluded from beam search\n",
    "                if costs[origin, j - 1] == -1:\n",
    "                    # set it back to np.inf so that it is more convenient to be recognized by pathfinder\n",
    "                    costs[origin, j - 1] = np.inf\n",
    "                else:\n",
    "                    prev_costs.append(transitions[i, origin] + costs[origin, j - 1])\n",
    "                    from_pts.append([origin, j - 1])\n",
    "\n",
    "            min_i = np.argmin(prev_costs)\n",
    "            origin = from_pts[min_i]\n",
    "            path_matrix[i, j] = origin\n",
    "            curr_cost = prev_costs[min_i] + states[i].evaluate(x[j])\n",
    "            costs[i, j] = min(costs[i, j], curr_cost)\n",
    "            i += 1\n",
    "        if not isinf(beam):\n",
    "            sort_idx = np.argsort(costs[:, j].flatten())\n",
    "            # cost is set to -1 if it is excluded from beam search\n",
    "            exclude_idx = sort_idx[beam:]\n",
    "            for i in exclude_idx:\n",
    "                if not isinf(costs[i, j]):\n",
    "                    costs[i, j] = -1\n",
    "        j += 1\n",
    "    # find the path\n",
    "    i = row_count - 1\n",
    "    j = col_count - 1\n",
    "    path = []\n",
    "    while i != 0 or j != 0:\n",
    "        i, j = path_matrix[i, j]\n",
    "        path.append([i, j])\n",
    "    return costs, np.array(path)\n",
    "\n",
    "\n",
    "def sentence_viterbi(x, model_graph):\n",
    "    # TODO: allow arbitrary jumps between gmm states depending on the hmm model\n",
    "    \"\"\"\n",
    "    :param x: input features.\n",
    "    :param model_graph: TODO: add docstring\n",
    "    :return: costs: cost matrix\n",
    "            matched_sentence: a sequence of index of models that best matches the input.\n",
    "\n",
    "    \"\"\"\n",
    "    nodes = model_graph.nodes\n",
    "\n",
    "    # initialize cost matrix\n",
    "    n_cols = len(x)\n",
    "    n_rows = len(nodes)\n",
    "    costs = np.full((n_rows, n_cols), np.inf)\n",
    "    costs[0, 0] = 0\n",
    "\n",
    "    path_matrix = np.full((n_rows, n_cols, 2), np.inf, dtype=np.int)\n",
    "\n",
    "    # the end of a sentence\n",
    "    end_nodes = model_graph.get_ends()\n",
    "\n",
    "    # fill cost matrix\n",
    "    for c in range(n_cols):\n",
    "        for r in range(0, n_rows):\n",
    "            if r == 0 and c == 0:\n",
    "                continue\n",
    "            subcosts = []\n",
    "            from_points = []\n",
    "\n",
    "            if model_graph[r].model_index == 'NES':\n",
    "                # the cost of non-emitting state is 0\n",
    "                node_dist = 0\n",
    "            else:\n",
    "                node_dist = model_graph[r].val.evaluate(x[c])\n",
    "\n",
    "            origins, transition_costs = model_graph.get_origins(model_graph[r])\n",
    "\n",
    "            for o, tc in zip(origins, transition_costs):\n",
    "                origin_idx = o.node_index\n",
    "                if model_graph[r].model_index == 'NES':\n",
    "                    subcosts.append(costs[origin_idx, c] + node_dist)\n",
    "                    from_points.append([origin_idx, c])\n",
    "                elif c > 0:\n",
    "                    subcosts.append(tc + costs[origin_idx, c - 1] + node_dist)\n",
    "                    from_points.append([origin_idx, c - 1])\n",
    "\n",
    "            # if there is no node that can get to current position, simply skip it.\n",
    "            if len(subcosts) == 0:\n",
    "                continue\n",
    "            # remember path and cost\n",
    "            min_idx = np.argmin(subcosts)\n",
    "            path_matrix[r, c] = from_points[min_idx]\n",
    "            costs[r, c] = subcosts[min_idx]\n",
    "\n",
    "    # find the sentence which has the min cost\n",
    "    end_node_costs = [costs[n.node_index, -1] for n in end_nodes]\n",
    "    min_idx = np.argmin(end_node_costs)\n",
    "    best_end_idx = end_nodes[min_idx].node_index\n",
    "    best_cost = costs[best_end_idx, -1]\n",
    "\n",
    "    # find the matched string\n",
    "    c = n_cols - 1\n",
    "    r = best_end_idx\n",
    "    matched_sentence = [nodes[r].model_index]\n",
    "    while 1:\n",
    "        if c < 1:\n",
    "            break\n",
    "        r, c = path_matrix[r, c]\n",
    "        if r != 0:\n",
    "            matched_sentence.append(nodes[r].model_index)\n",
    "\n",
    "    return best_cost, matched_sentence[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# kmeans.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_variance(data):\n",
    "    return np.cov(data).diagonal()\n",
    "\n",
    "\n",
    "def combine_templates(templates, n_temps, n_segments, seg_starts):\n",
    "    \"\"\"\n",
    "    :param templates: a list of templates\n",
    "    :param n_temps: the number of templates\n",
    "    :param n_segments: the number of segments the final templates will have\n",
    "    :param seg_starts: the start points of the segments in the original templates\n",
    "    :return: the combined templates\n",
    "    \"\"\"\n",
    "    res = np.zeros((n_segments, templates[0].shape[1]))\n",
    "    vars = np.zeros((n_segments, templates[0].shape[1]))\n",
    "    segments = segment_data(templates, n_temps, n_segments, seg_starts)\n",
    "    for s in range(n_segments):\n",
    "        seg = segments[s]\n",
    "        res[s] = seg.mean(axis=0)\n",
    "        vars[s] = calc_variance(seg.T)\n",
    "    return res, vars\n",
    "\n",
    "\n",
    "def segment_data(templates, n_temps, n_segments, seg_starts):\n",
    "    \"\"\"\n",
    "    :param templates: a list of templates\n",
    "    :param n_temps: the number of templates\n",
    "    :param n_segments: the number of segments the final templates will have\n",
    "    :param seg_starts: the start points of the segments in the original templates\n",
    "    :return: the segmented data\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for s in range(n_segments):\n",
    "        seg = []\n",
    "        for r in range(n_temps):\n",
    "            if s == n_segments - 1:\n",
    "                seg += templates[r][seg_starts[r, s]:].tolist()\n",
    "            else:\n",
    "                seg += templates[r][seg_starts[r, s]:seg_starts[r, s + 1]].tolist()\n",
    "        segments.append(np.array(seg))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def calc_transition_costs(templates, seg_lens, max_jump_dist=2):\n",
    "    \"\"\"\n",
    "    Calculate and construct a dictionary used in dtw()\n",
    "    :param templates: a list of templates\n",
    "    :param seg_lens: list of length of segments of all templates\n",
    "    :param max_jump_dist: maximum distance allowed for a transition to jump across, the default value 2 is optimal in\n",
    "        most occasions.\n",
    "    :return: the transition cost\n",
    "    \"\"\"\n",
    "    n_segments = seg_lens.shape[1]\n",
    "    n_temps = len(templates)\n",
    "    empty_segs = (seg_lens == 0)\n",
    "    res = np.full((n_segments, n_segments), np.inf)\n",
    "    for i in range(n_segments):\n",
    "        jump_dist = 1\n",
    "        # number of samples which transition to other segments\n",
    "        if i == n_segments - 1:\n",
    "            n_jump = 0\n",
    "        else:\n",
    "            n_jump = n_temps\n",
    "        s = i + 1\n",
    "        # calculate the distance of the jump\n",
    "        while s < n_segments - 1:\n",
    "            # if the next segments is not empty, the jump stops here; otherwise increment jump_dist\n",
    "            if np.sum(empty_segs[:, s + 1]) == 0:\n",
    "                break\n",
    "            jump_dist += 1\n",
    "            if jump_dist > max_jump_dist:\n",
    "                break\n",
    "            s += 1\n",
    "        # number of samples in this segment\n",
    "        n_all = 0\n",
    "        for t in range(len(templates)):\n",
    "            n_all += seg_lens[t, i]\n",
    "        # number of samples which transition to itself\n",
    "        n_stay = n_all - n_jump\n",
    "        p_stay = n_stay / n_all\n",
    "        p_jump = n_jump / n_all\n",
    "        if n_jump:\n",
    "            res[i + jump_dist, i] = -np.log(p_jump)\n",
    "        res[i, i] = -np.log(p_stay)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_segments_from_path(path, n_segments):\n",
    "    # count all segments occurrence\n",
    "    unique, counts = np.unique(path[:, 0], return_counts=True)\n",
    "    seg_counts = dict(zip([i for i in range(n_segments)], [0 for _ in range(n_segments)]))\n",
    "    seg_counts.update(dict(zip(unique, counts)))\n",
    "    counts = np.array(list(seg_counts.values()))\n",
    "    unique = np.array(list(seg_counts.keys()))\n",
    "    sort_idx = np.argsort(unique)\n",
    "    counts = counts[sort_idx]\n",
    "    seg_starts = np.add.accumulate(counts)[:-1]\n",
    "    return seg_starts\n",
    "\n",
    "\n",
    "def skmeans(templates, n_segments, dist_fun=lambda *args: np.linalg.norm(args[0] - args[1]),\n",
    "            return_segmented_data=False, max_iteration=1000):\n",
    "    \"\"\"\n",
    "    :param templates: templates as a list\n",
    "    :param n_segments: number of segments in the final template\n",
    "    :param dist_fun: function to calculate distance of two nodes in the templates\n",
    "    :param return_segmented_data: if true, return segmented data, and vice versa.\n",
    "    :param max_iteration: max iteration if not converged\n",
    "    :return: res: a combined template;\n",
    "            vars: the variance of all segments;\n",
    "            transition_costs: cost matrix of transitions;\n",
    "            if return_segmented_data is set to true, segmented_data is returned, it is a list of all segments.\n",
    "    \"\"\"\n",
    "    assert max_iteration > 0\n",
    "    n_temps = len(templates)\n",
    "    seg_lens = np.zeros((n_temps, n_segments + 1), dtype=np.int)\n",
    "    for r in range(n_temps):\n",
    "        temp_len = len(templates[r])\n",
    "        seg_len = temp_len // n_segments\n",
    "        seg_lens[r, 1:] = seg_len\n",
    "    seg_starts = np.add.accumulate(seg_lens, axis=1)[:, :-1]\n",
    "    seg_lens = seg_lens[:, 1:]\n",
    "\n",
    "    res, vars = combine_templates(templates, n_temps, n_segments, seg_starts)\n",
    "    for _ in range(max_iteration):\n",
    "        # do dtw and align templates\n",
    "        seg_starts = np.zeros((n_temps, n_segments), dtype=np.int)\n",
    "        transition_costs = calc_transition_costs(templates, seg_lens)\n",
    "        for r in range(n_temps):\n",
    "            # if template contains too few mfcc vectors, we cannot find a path in dtw\n",
    "            if templates[r].shape[0] < 5:\n",
    "                raise NameError('template is too small, cannot do dtw on it')\n",
    "\n",
    "            _, path = dtw(templates[r], res, dist_fun, transition_costs)\n",
    "            seg_starts[r, 1:] = get_segments_from_path(path, n_segments)\n",
    "        new_res, vars = combine_templates(templates, n_temps, n_segments, seg_starts)\n",
    "        if np.allclose(res, new_res):\n",
    "            break\n",
    "        res = new_res\n",
    "    if return_segmented_data:\n",
    "        segmented_data = segment_data(templates, n_temps, n_segments, seg_starts)\n",
    "        return res, vars, transition_costs, segmented_data\n",
    "    else:\n",
    "        return res, vars, transition_costs\n",
    "\n",
    "\n",
    "def cluster_centroids(data, clusters, k):\n",
    "    \"\"\"Return centroids of clusters in data.\n",
    "    \"\"\"\n",
    "    result = np.empty(shape=(k,) + data.shape[1:])\n",
    "    for i in range(k):\n",
    "        np.mean(data[clusters == i, :], axis=0, out=result[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "def kmeans(data, k, centroids, dist_fun=lambda *args: np.linalg.norm(args[0] - args[1]), max_iteration=1000):\n",
    "    \"\"\"Modified k-means algorithm to fit the scenario\n",
    "    \"\"\"\n",
    "    assert k == centroids.shape[0]\n",
    "    clusters = np.random.randint(0, k, data.shape[0])\n",
    "    # find the covariance matrix of the new clusters\n",
    "    cov = []\n",
    "    for c in range(k):\n",
    "        segments = data[clusters == c]\n",
    "        cov.append(calc_variance(segments.T))\n",
    "    cov = np.array(cov)\n",
    "    # calculate distance from centroids\n",
    "    for _ in range(max(max_iteration, 1)):\n",
    "        dists = np.zeros((data.shape[0], k))\n",
    "        for i in range(data.shape[0]):\n",
    "            for c in range(k):\n",
    "                dists[i, c] = dist_fun(centroids[c, :], data[i, :], cov[0])\n",
    "        dists = np.array(dists)\n",
    "        # Index of the closest centroid to each data point.\n",
    "        clusters = np.argmin(dists, axis=1)\n",
    "        new_centroids = cluster_centroids(data, clusters, k)\n",
    "\n",
    "        # check convergence\n",
    "        if np.array_equal(new_centroids, centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return clusters, centroids, cov\n",
    "\n",
    "\n",
    "def align_gmm_states(templates, gmm_states, transition_costs, n_segments):\n",
    "    n_temps = len(templates)\n",
    "\n",
    "    # do dtw and align templates\n",
    "    seg_starts = np.zeros((n_temps, n_segments), dtype=np.int)\n",
    "    for r in range(n_temps):\n",
    "        t = templates[r]\n",
    "        _, path = dtw1(t, gmm_states, transition_costs)\n",
    "        seg_starts[r, 1:] = get_segments_from_path(path, n_segments)\n",
    "    return segment_data(templates, n_temps, n_segments, seg_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm.py\n",
    "import numpy as np\n",
    "import copy\n",
    "import scipy\n",
    "\n",
    "\n",
    "class MultivariateNormal:\n",
    "    def __init__(self, mean, cov):\n",
    "        self.mean = mean\n",
    "        self._cov = cov\n",
    "        if len(self.cov.shape) == 1:\n",
    "            cov = np.diag(self.cov)\n",
    "        else:\n",
    "            cov = self.cov\n",
    "        self.inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "    @property\n",
    "    def cov(self):\n",
    "        return self._cov\n",
    "\n",
    "    @cov.setter\n",
    "    def cov(self, val):\n",
    "        self._cov = val\n",
    "        if len(self.cov.shape) == 1:\n",
    "            cov = np.diag(self.cov)\n",
    "        else:\n",
    "            cov = self.cov\n",
    "        self.inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "    @cov.deleter\n",
    "    def cov(self):\n",
    "        del self.cov\n",
    "\n",
    "    def pdf(self, x):\n",
    "        size = x.shape[0]\n",
    "        if size == self.mean.shape[0]:\n",
    "            det = np.prod(self.cov)\n",
    "            norm_const = 1.0 / (np.power((2 * np.pi), float(size) / 2) * np.sqrt(det))\n",
    "            x_mu = x - self.mean\n",
    "            result = np.exp(-0.5 * (x_mu.dot(self.inv_cov).dot(x_mu.T)))\n",
    "            return norm_const * result\n",
    "        else:\n",
    "            raise NameError(\"The dimensions of the input don't match\")\n",
    "\n",
    "\n",
    "def dist(v1, v2, variance):\n",
    "    D = len(variance)\n",
    "    m = (v1 - v2)\n",
    "    return 0.5 * np.log((2 * np.pi) ** D * np.prod(variance)) + 0.5 * np.sum(m / variance * m)\n",
    "\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, mu, sigma, n_gaussians):\n",
    "        self.n_gaussians = n_gaussians\n",
    "        self.w = np.full(n_gaussians, 1 / n_gaussians)\n",
    "        self.dists = [MultivariateNormal(mean=mu, cov=sigma) for _ in range(n_gaussians)]\n",
    "        self.mu_old = np.tile(mu, (n_gaussians, 1))\n",
    "        self.sigma_old = np.tile(sigma, (n_gaussians, 1))\n",
    "        self.w_old = np.full(n_gaussians, 1 / n_gaussians)\n",
    "\n",
    "    def evaluate(self, x, return_neg_log_likelihood=True):\n",
    "        res = [self.dists[i].pdf(x) * self.w[i] for i in range(self.n_gaussians)]\n",
    "        res = np.array(res)\n",
    "        if return_neg_log_likelihood:\n",
    "            return -np.log(res.sum())\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def em(self, data, n_gaussians, max_iteration=10000):\n",
    "        for iter in range(max_iteration):\n",
    "            p = np.zeros((data.shape[0], n_gaussians))\n",
    "            mu = np.zeros((data.shape[1], n_gaussians))\n",
    "            sigma = np.zeros((data.shape[1], n_gaussians))\n",
    "            for i in range(data.shape[0]):\n",
    "                p[i, :] = self.evaluate(data[i, :], return_neg_log_likelihood=False)[:n_gaussians]\n",
    "\n",
    "            p_sum = np.sum(p, axis=1).reshape((p.shape[0], 1))\n",
    "            # avoid divide by 0\n",
    "            p_sum[p_sum == 0] = 10 ** (-5)\n",
    "            p /= p_sum\n",
    "            p_sum = np.sum(p, axis=0)\n",
    "            # avoid divide by 0\n",
    "            p_sum[p_sum == 0] = 10 ** (-5)\n",
    "            for c in range(n_gaussians):\n",
    "                mu[:, c] = np.sum(data * p[:, [c]], axis=0)\n",
    "                mu[:, c] /= p_sum[c]\n",
    "                # sigma\n",
    "                sub2 = (data - mu[:, c]) ** 2\n",
    "                sigma[:, c] = np.sum(sub2 * p[:, [c]], axis=0)\n",
    "                sigma[:, c] /= p_sum[c]\n",
    "\n",
    "            mu = mu.T\n",
    "            sigma = sigma.T\n",
    "            # update w and mu\n",
    "            w = p.mean(axis=0).T\n",
    "            self.update_models(mu, sigma, w)\n",
    "            if np.allclose(mu, self.mu_old[:n_gaussians, :]) and \\\n",
    "                    np.allclose(sigma, self.sigma_old[:n_gaussians, :]) and \\\n",
    "                    np.allclose(w, self.w_old[:n_gaussians]):\n",
    "                print(\"EM converged at iteration:\", iter)\n",
    "                break\n",
    "            else:\n",
    "                print(\"EM iteration:\", str(iter), end=\"\\r\")\n",
    "                self.mu_old[:n_gaussians, :] = mu\n",
    "                self.sigma_old[:n_gaussians, :] = sigma\n",
    "                self.w_old[:n_gaussians] = w\n",
    "\n",
    "    def update_models(self, mus, sigmas, weights):\n",
    "        self.w[:mus.shape[0]] = weights\n",
    "        for m in range(mus.shape[0]):\n",
    "            self.dists[m].mean = mus[m, :]\n",
    "            self.dists[m].cov = sigmas[m, :]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        res = True\n",
    "        res = res and self.n_gaussians == other.n_gaussians and np.allclose(self.w, other.w)\n",
    "        for i in range(self.n_gaussians):\n",
    "            res = res and np.allclose(self.dists[i].mean, other.dists[i].mean) and np.allclose(self.dists[i].cov,\n",
    "                                                                                               other.dists[i].cov)\n",
    "        return res\n",
    "\n",
    "\n",
    "class HMM:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    mu: an array of means of all segments\n",
    "\n",
    "    sigma: an array of variance of all segments\n",
    "    transitions: transition matrix\n",
    "\n",
    "    segments: a list of mfcc feature vectors of each segments\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_segments):\n",
    "        self.n_segments = n_segments\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.transitions = None\n",
    "        self.segments = []\n",
    "        self.gmm_states = None\n",
    "        self.use_gmm = True\n",
    "        self.use_em = True\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.use_gmm:\n",
    "            if not other.use_gmm:\n",
    "                return False\n",
    "            if self.n_segments != other.n_segments:\n",
    "                return False\n",
    "            for i in range(self.n_segments):\n",
    "                if self.gmm_states[i] != other.gmm_states[i]:\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return np.allclose(self.mu, other.mu) and np.allclose(self.sigma, other.sigma)\n",
    "\n",
    "    def reset(self):\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.transitions = None\n",
    "        self.segments = []\n",
    "        self.gmm_states = None\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if type(item) is int or type(item) is slice:\n",
    "            return self.gmm_states[item]\n",
    "        else:\n",
    "            raise TypeError('The type of index is not supported')\n",
    "\n",
    "    def fit(self, ys, n_gaussians, use_gmm=True, use_em=True):\n",
    "        \"\"\"Fit the HMM model with a list of training data.\n",
    "        :param use_gmm: TODO: add docstring\n",
    "        :param use_em: if true, use both k-means and Expectation-Maximization algorithm to fit GMM, otherwise only\n",
    "            use k-means to do it.\n",
    "        :param ys: a list of mfcc templates. It cannot be a numpy array since variable length rows in a matrix are\n",
    "            not supported.\n",
    "        :param n_gaussians: the number of gaussian distributions. It should be some power of 2, if not it will be\n",
    "            converted to previous power of 2.\n",
    "        :return: the trained HMM model.\n",
    "        \"\"\"\n",
    "        self.use_em = use_em\n",
    "        self.use_gmm = use_gmm\n",
    "        if use_gmm:\n",
    "            self.fit_GMM(ys, n_gaussians)\n",
    "        else:\n",
    "            self.mu, self.sigma, self.transitions, self.segments = skmeans(ys, self.n_segments,\n",
    "                                                                           return_segmented_data=True)\n",
    "        return self\n",
    "\n",
    "    def _init_gmm(self, n_gaussians):\n",
    "        self.gmm_states = [GMM(self.mu[i, :], self.sigma[i, :], n_gaussians) for i in range(self.n_segments)]\n",
    "\n",
    "    def fit_GMM(self, ys, n_gaussians):\n",
    "        \"\"\"fit all GMM states in the HMM.\n",
    "        :param n_gaussians: the number of gaussians.\n",
    "        \"\"\"\n",
    "        print('Doing segmental k-means')\n",
    "        self.mu, self.sigma, self.transitions, self.segments = skmeans(ys, self.n_segments,\n",
    "                                                                       return_segmented_data=True)\n",
    "        self._init_gmm(n_gaussians)\n",
    "\n",
    "        # train each GMM\n",
    "        for i in range(len(self.segments)):\n",
    "            self._fit_GMM(self.segments[i], n_gaussians, i)\n",
    "\n",
    "        # update segments\n",
    "        self.segments = align_gmm_states(ys, self.gmm_states, self.transitions, self.n_segments)\n",
    "\n",
    "    def _fit_GMM(self, data, n_gaussians, seg_i):\n",
    "        \"\"\"fit a single GMM state. Only for internal use.\n",
    "        :param data: the data only for this state.\n",
    "        :param n_gaussians: the number of gaussians.\n",
    "        :param seg_i: the index of this segment/state.\n",
    "        :return: a GMM object.\n",
    "        \"\"\"\n",
    "        n_splits = int(np.log(n_gaussians))\n",
    "        assert n_splits > 0\n",
    "\n",
    "        centroids = np.array([self.mu[seg_i, :]])\n",
    "        weights = np.full(n_gaussians, 1 / data.shape[0])\n",
    "        for i in range(n_splits):\n",
    "            lcentroids = centroids * 0.9\n",
    "            hcentroids = centroids * 1.1\n",
    "            centroids = np.concatenate([lcentroids, hcentroids], axis=0)\n",
    "            clusters, centroids, variance = kmeans(data, 2 ** (i + 1), centroids, dist_fun=dist)\n",
    "\n",
    "            # calculate and update weights of distributions\n",
    "            cs, c_counts = np.unique(clusters, return_counts=True)\n",
    "            for c in cs:\n",
    "                weights[c] = c_counts[c] / data.shape[0]\n",
    "\n",
    "            # update model parameters\n",
    "            self.gmm_states[seg_i].update_models(centroids, variance, weights[:2 ** (i + 1)])\n",
    "            # Expectation-maximization\n",
    "            if self.use_em:\n",
    "                self.gmm_states[seg_i].em(data, 2 ** (i + 1))\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input data.\n",
    "        :return: cost/score/probability.\n",
    "        \"\"\"\n",
    "        if self.use_gmm:\n",
    "            costs, _ = dtw1(x, self.gmm_states, self.transitions)\n",
    "        else:\n",
    "            costs, _ = dtw(x, self.mu, dist, self.transitions, self.sigma)\n",
    "        return costs[-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_graph.py\n",
    "import uuid\n",
    "\n",
    "\n",
    "class GraphNode:\n",
    "    def __init__(self, val, model_index=None, node_index=None):\n",
    "        self.val = val\n",
    "        self.model_index = model_index\n",
    "        self.node_index = node_index\n",
    "        # help distinguish different nodes\n",
    "        self.id = uuid.uuid4().int\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "\n",
    "\n",
    "# TODO: make apis of LayeredHMMGraph ContinuousGraph the same\n",
    "class Graph:\n",
    "    def __init__(self, nodes):\n",
    "        self.nodes = nodes\n",
    "        self.edges = []\n",
    "\n",
    "    def add_edge(self, from_, to, val=0):\n",
    "        if from_ not in self.nodes:\n",
    "            self.nodes.append(from_)\n",
    "        if to not in self.nodes:\n",
    "            self.nodes.append(to)\n",
    "        self.edges.append((from_, to, val))\n",
    "\n",
    "    def get_dests(self, origin):\n",
    "        res = []\n",
    "        values = []\n",
    "        for o, d, val in self.edges:\n",
    "            if o.id == origin.id:\n",
    "                res.append(d)\n",
    "                values.append(val)\n",
    "        return res, values\n",
    "\n",
    "    def get_origins(self, dest):\n",
    "        res = []\n",
    "        values = []\n",
    "        for o, d, val in self.edges:\n",
    "            if d.id == dest.id:\n",
    "                res.append(o)\n",
    "                values.append(val)\n",
    "        return res, values\n",
    "\n",
    "    def get_ends(self):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.nodes[item]\n",
    "\n",
    "\n",
    "class LayeredHMMGraph(Graph):\n",
    "    def __init__(self, nodes):\n",
    "        super().__init__(nodes)\n",
    "        self.curr_layer = []\n",
    "        self.ends = []\n",
    "\n",
    "    def add_non_emitting_state(self, end=False):\n",
    "        nes = GraphNode(None, model_index='NES', node_index=len(self.nodes))\n",
    "        self.nodes.append(nes)  # NES stands for non-emitting state\n",
    "        if len(self.curr_layer) > 0:\n",
    "            for m in self.curr_layer:\n",
    "                self.add_edge(m, nes)\n",
    "        self.curr_layer = [nes]\n",
    "        if end:\n",
    "            self.ends.append(nes)\n",
    "        return nes\n",
    "\n",
    "    def add_layer_from_models(self, models):\n",
    "        # assert that the previous layer is a non-emitting state\n",
    "        assert len(self.curr_layer) == 1 and self.curr_layer[0].model_index == 'NES'\n",
    "        new_layer = []\n",
    "        for i in range(len(models)):\n",
    "            m = models[i]\n",
    "            # get gmm states of all models\n",
    "            gmm_nodes = [GraphNode(gs, model_index=i) for gs in m.gmm_states]\n",
    "            n_gaussians = len(gmm_nodes)\n",
    "            # connect previous layer to the current one\n",
    "            self.add_edge(self.curr_layer[0], gmm_nodes[0], 0)\n",
    "\n",
    "            # build graph for all gmm states in each hmm model\n",
    "            for j in range(0, n_gaussians):\n",
    "                # self loop\n",
    "                self.add_edge(gmm_nodes[j], gmm_nodes[j], val=m.transitions[j, j])\n",
    "                if j < n_gaussians - 1:\n",
    "                    # connect to the next\n",
    "                    self.add_edge(gmm_nodes[j], gmm_nodes[j + 1], val=m.transitions[j + 1, j])\n",
    "                else:\n",
    "                    new_layer.append(gmm_nodes[j])\n",
    "        # update self.curr_layer to the last gmm_states in models\n",
    "        self.curr_layer = new_layer\n",
    "        self.update_node_indices()\n",
    "        return new_layer\n",
    "\n",
    "    # TODO: don't update all nodes when unnecessary\n",
    "    def update_node_indices(self):\n",
    "        n_nodes = len(self.nodes)\n",
    "        for i in range(n_nodes):\n",
    "            if self.nodes[i].node_index is not None:\n",
    "                assert self.nodes[i].node_index == i\n",
    "            self.nodes[i].node_index = i\n",
    "\n",
    "    def get_ends(self):\n",
    "        return self.ends\n",
    "\n",
    "\n",
    "class ContinuousGraph(Graph):\n",
    "    def __init__(self, nodes):\n",
    "        super().__init__(nodes)\n",
    "        self.current = None\n",
    "\n",
    "    def add_non_emitting_state(self):\n",
    "        nes = GraphNode(None, 'NES')\n",
    "        self.nodes.append(nes)  # NES stands for non-emitting state\n",
    "        if self.current is not None:\n",
    "            self.add_edge(self.current, nes)\n",
    "        self.current = nes\n",
    "        # set node_index\n",
    "        nes.node_index = len(self.nodes) - 1\n",
    "        return nes\n",
    "\n",
    "    def add_model(self, model, model_index):\n",
    "        # get gmm states of all models\n",
    "        gmm_nodes = [GraphNode(model.gmm_states[i], model_index=model_index) for i in\n",
    "                     range(len(model.gmm_states))]\n",
    "        n_segments = len(gmm_nodes)\n",
    "        # connect previous layer to the current one\n",
    "        self.add_edge(self.current, gmm_nodes[0], val=0)\n",
    "\n",
    "        # build graph for all gmm states in each hmm model\n",
    "        for j in range(0, n_segments):\n",
    "            # self loop\n",
    "            self.add_edge(gmm_nodes[j], gmm_nodes[j], val=model.transitions[j, j])\n",
    "            if j < n_segments - 1:\n",
    "                # connect to the next\n",
    "                self.add_edge(gmm_nodes[j], gmm_nodes[j + 1], val=model.transitions[j + 1, j])\n",
    "        self.current = gmm_nodes[-1]\n",
    "\n",
    "        # update node_index in all nodes\n",
    "        self.update_node_indices()\n",
    "        return self.current\n",
    "\n",
    "    def update_node_indices(self):\n",
    "        n_nodes = len(self.nodes)\n",
    "        for i in range(n_nodes):\n",
    "            if self.nodes[i].node_index is not None:\n",
    "                assert self.nodes[i].node_index == i\n",
    "            self.nodes[i].node_index = i\n",
    "\n",
    "    def get_ends(self):\n",
    "        return [self.current]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous_train.py\n",
    "from itertools import chain\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "\n",
    "def continuous_train(data, models, lables, use_gmm=True, n_gaussians=4, use_em=True, max_iteration=1000):\n",
    "    # remember old models\n",
    "    old_models = copy.deepcopy(models)\n",
    "\n",
    "    for iter in range(max_iteration):\n",
    "        converged = True\n",
    "        print('=' * 25)\n",
    "        print('Continuous training iteration:', iter)\n",
    "        print('Rearranging data for hmm training...')\n",
    "        segments = {w: [] for w in chain.from_iterable(lables)}\n",
    "        data_len = len(data)\n",
    "\n",
    "        print('Building model graphs')\n",
    "        # make model_graphs for current models\n",
    "        model_graphs = [ContinuousGraph([]) for _ in lables]\n",
    "        n_labels = len(lables)\n",
    "        for i in range(n_labels):\n",
    "            lb = lables[i]\n",
    "            model_graphs[i].add_non_emitting_state()\n",
    "            for digit in lb:\n",
    "                model_graphs[i].add_model(old_models[digit], digit)\n",
    "                model_graphs[i].add_non_emitting_state()\n",
    "\n",
    "        for i in range(data_len):\n",
    "            m = model_graphs[i]\n",
    "            x = data[i]\n",
    "\n",
    "            _, path = sentence_viterbi(x, m)\n",
    "            print('Progress:', str(int(100 * i / data_len)) + \"%\", end='\\r')\n",
    "\n",
    "            # find segments for every digit, a segment is ended and started by 'NES'\n",
    "            # each segment is a sequence of mfcc features, thus it's a 2d array-like\n",
    "            i_nes = [i for i, x in enumerate(path) if x == \"NES\"]\n",
    "            i_nes_len = len(i_nes)\n",
    "            for index in range(i_nes_len - 1):\n",
    "                start_i = i_nes[index] + 1\n",
    "                segments[path[start_i]].append(x[start_i:i_nes[index + 1]])\n",
    "\n",
    "        print('Complete data rearrangement')\n",
    "        print(\"=\" * 25)\n",
    "\n",
    "        # remove templates that are too small to do dtw on\n",
    "        for w in segments.keys():\n",
    "            seg = segments[w]\n",
    "            seg_len = len(seg)\n",
    "            del_i = []\n",
    "            for si in range(seg_len):\n",
    "                if seg[si].shape[0] < 5:\n",
    "                    warnings.warn('Removing digit templates that are too small', UserWarning)\n",
    "                    del_i.append(si)\n",
    "\n",
    "            del_i.sort(reverse=True)\n",
    "            for si in del_i:\n",
    "                del seg[si]\n",
    "\n",
    "        # continuous training\n",
    "        print('Doing HMM training...')\n",
    "        for w in segments.keys():\n",
    "            # train a new HMM model using the segments\n",
    "            m = HMM(5)\n",
    "            m.fit(segments[w], n_gaussians, use_gmm, use_em)\n",
    "            converged = converged and m == old_models[w]\n",
    "            old_models[w] = m\n",
    "            # TODO: use command line argument for output model path\n",
    "            file = open(os.path.join(continuous_model_path, str(w) + '.pkl'), 'wb')\n",
    "            pickle.dump(m, file)\n",
    "            file.close()\n",
    "\n",
    "        if converged:\n",
    "            print('Continuous training converged')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core.py\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import python_speech_features as psf\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def delta_feature(feat):\n",
    "    delta = np.zeros(feat.shape)\n",
    "    for i in range(len(feat)):\n",
    "        if i == 0:\n",
    "            delta[i] = feat[i + 1] - feat[i]\n",
    "        elif i == len(feat) - 1:\n",
    "            delta[i] = feat[i] - feat[i - 1]\n",
    "        else:\n",
    "            delta[i] = feat[i + 1] - feat[i - 1]\n",
    "    return delta\n",
    "\n",
    "def load_wav_as_mfcc(path):\n",
    "    \"\"\"\n",
    "    Another version of load_wav_as_mfcc using library python_speech_feature to calculate\n",
    "    mfcc, to check if this implementation is correct.\n",
    "    \"\"\"\n",
    "    sample_rate, signal = wavfile.read(path)\n",
    "    mfcc = psf.mfcc(signal, sample_rate, nfilt=40, preemph=0.95, appendEnergy=False, winfunc=np.hamming)\n",
    "    df = delta_feature(mfcc)\n",
    "    ddf = delta_feature(df)\n",
    "    features = np.concatenate([mfcc, df, ddf], axis=1)\n",
    "    features = standardize(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def make_HMM(filenames, n_segs, use_gmm, use_em):\n",
    "    print('Loading wav files to mfcc features')\n",
    "    ys = [load_wav_as_mfcc(filename) for filename in filenames]\n",
    "    m = HMM(n_segs)\n",
    "    print('Fitting HMMs')\n",
    "    model = m.fit(ys, n_gaussians=4, use_gmm=use_gmm, use_em=use_em)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(filenames, model_folder, model_name, n_segs, use_gmm, use_em):\n",
    "    models = make_HMM(filenames, n_segs, use_gmm=use_gmm, use_em=use_em)\n",
    "    file = open(os.path.join(model_folder, model_name + '.pkl'), 'wb')\n",
    "    pickle.dump(models, file)\n",
    "\n",
    "\n",
    "def test(models, folder, file_patterns):\n",
    "    n_passed = 0\n",
    "    n_tests = 0\n",
    "\n",
    "    # get the best model for every digit\n",
    "    for digit in range(len(digit_names)):\n",
    "        # get all test files using regular expresssions\n",
    "        filenames = [os.path.join(folder, file) for file in os.listdir(folder) if\n",
    "                     re.match(file_patterns[digit], file)]\n",
    "        # update the total number of tests\n",
    "        n_tests += len(filenames)\n",
    "\n",
    "        # do evaluation on all models, find the best one\n",
    "        # NOTE: the evaluation is based on costs\n",
    "        for f in filenames:\n",
    "            input = load_wav_as_mfcc(f)\n",
    "            best_model = 0\n",
    "            c = np.inf\n",
    "            # find the best model\n",
    "            for i in range(len(models)):\n",
    "                m = models[i]\n",
    "                cost = m.evaluate(input)\n",
    "                if cost < c:\n",
    "                    c = cost\n",
    "                    best_model = i\n",
    "            # if the best model is correct, move on\n",
    "            if best_model == digit:\n",
    "                n_passed += 1\n",
    "            # if the best model is wrong, log info\n",
    "            else:\n",
    "                print(\"Digit:\", digit_names[digit], \"is wrong\")\n",
    "    return n_passed / n_tests\n",
    "\n",
    "\n",
    "def aurora_continuous_train():\n",
    "    models = []\n",
    "    for digit in digit_names:\n",
    "        # TODO: use command line argument for input model path\n",
    "        file = open(os.path.join(isolated_model_path, str(digit) + '.pkl'), 'rb')\n",
    "        models.append(pickle.load(file))\n",
    "        file.close()\n",
    "\n",
    "    # get filenames\n",
    "    sequence_regex = re.compile('(?<=_)[OZ0-9]+(?=[AB])')\n",
    "    filenames = [f for f in os.listdir(os.path.join(data_path, 'train')) if os.path.isfile(os.path.join(data_path, 'train', f))]\n",
    "    filenames.sort()\n",
    "\n",
    "    # get transcripts\n",
    "    sequences = [re.search(sequence_regex, f).group(0) for f in filenames]\n",
    "    labels = [list(map(lambda x: filename_index_map[x], s)) for s in sequences]\n",
    "\n",
    "\n",
    "    print('loading data')\n",
    "    data = [load_wav_as_mfcc(os.path.join(data_path, 'train', f)) for f in filenames]\n",
    "\n",
    "    continuous_train(data, models, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Doing isolated training...')\n",
    "# # data folder location\n",
    "# folder = os.path.join(data_path, 'train')\n",
    "# for digit in digit_names:\n",
    "#     filenames = [os.path.join(folder, f) for f in os.listdir(folder) if\n",
    "#                  re.match('[A-Z]+_' + digit + '[AB].wav', f)]\n",
    "# \n",
    "#     train(filenames, isolated_model_path, digit, n_segs=5, use_gmm=True, use_em=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aurora_continuous_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
